{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:08:27.498648Z",
     "start_time": "2023-06-28T19:08:26.775363Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:09:32.869680Z",
     "start_time": "2023-06-28T19:09:32.862375Z"
    }
   },
   "outputs": [],
   "source": [
    "pages = [str(i) for i in range(0, 1, 1)] # page iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:09:36.176087Z",
     "start_time": "2023-06-28T19:09:36.165151Z"
    }
   },
   "outputs": [],
   "source": [
    "page_urls = []\n",
    "for page in pages:\n",
    "    base_url = 'https://www.ted.com/talks?page='+page\n",
    "    page_urls.append(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:09:37.916956Z",
     "start_time": "2023-06-28T19:09:37.908048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['https://www.ted.com/talks?page=0']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:10:08.652021Z",
     "start_time": "2023-06-28T19:09:43.984651Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading page 1/5: : 63it [00:52,  1.20it/s][A\n",
      "\n",
      "100%|\u001B[32m██████████\u001B[0m| 1/1 [00:01<00:00,  1.14s/it]\u001B[A\n",
      "Downloading page 1/1: 100%|\u001B[32m██████████\u001B[0m| 1/1 [00:01<00:00,  1.14s/it]\u001B[A\n",
      "Downloading page 1/1: : 2it [00:01,  1.31it/s]                     \u001B[A\n",
      "Downloading page 1/1: : 2it [00:01,  1.31it/s]\u001B[A\n",
      "Downloading page 1/1: : 3it [00:02,  1.54it/s]\u001B[A\n",
      "Downloading page 1/1: : 3it [00:02,  1.54it/s]\u001B[A\n",
      "Downloading page 1/1: : 4it [00:02,  1.68it/s]\u001B[A\n",
      "Downloading page 1/1: : 4it [00:02,  1.68it/s]\u001B[A\n",
      "Downloading page 1/1: : 5it [00:03,  1.77it/s]\u001B[A\n",
      "Downloading page 1/1: : 5it [00:03,  1.77it/s]\u001B[A\n",
      "Downloading page 1/1: : 6it [00:03,  1.83it/s]\u001B[A\n",
      "Downloading page 1/1: : 6it [00:03,  1.83it/s]\u001B[A\n",
      "Downloading page 1/1: : 7it [00:04,  1.87it/s]\u001B[A\n",
      "Downloading page 1/1: : 7it [00:04,  1.87it/s]\u001B[A\n",
      "Downloading page 1/1: : 8it [00:04,  1.91it/s]\u001B[A\n",
      "Downloading page 1/1: : 8it [00:04,  1.91it/s]\u001B[A\n",
      "Downloading page 1/1: : 9it [00:05,  1.93it/s]\u001B[A\n",
      "Downloading page 1/1: : 9it [00:05,  1.93it/s]\u001B[A\n",
      "Downloading page 1/1: : 10it [00:05,  1.94it/s]\u001B[A\n",
      "Downloading page 1/1: : 10it [00:05,  1.94it/s]\u001B[A\n",
      "Downloading page 1/1: : 11it [00:06,  1.95it/s]\u001B[A\n",
      "Downloading page 1/1: : 11it [00:06,  1.95it/s]\u001B[A\n",
      "Downloading page 1/1: : 12it [00:06,  1.95it/s]\u001B[A\n",
      "Downloading page 1/1: : 12it [00:06,  1.95it/s]\u001B[A\n",
      "Downloading page 1/1: : 13it [00:07,  1.95it/s]\u001B[A\n",
      "Downloading page 1/1: : 13it [00:07,  1.95it/s]\u001B[A\n",
      "Downloading page 1/1: : 14it [00:07,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 14it [00:07,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 15it [00:08,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 15it [00:08,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 16it [00:08,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 16it [00:08,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 17it [00:09,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 17it [00:09,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 18it [00:09,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 18it [00:09,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 19it [00:10,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 19it [00:10,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 20it [00:10,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 20it [00:10,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 21it [00:11,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 21it [00:11,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 22it [00:11,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 22it [00:11,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 23it [00:12,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 23it [00:12,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 24it [00:12,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 24it [00:12,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 25it [00:13,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 25it [00:13,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 26it [00:13,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 26it [00:13,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 27it [00:14,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 27it [00:14,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 28it [00:14,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 28it [00:14,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 29it [00:15,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 29it [00:15,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 30it [00:15,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 30it [00:15,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 31it [00:16,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 31it [00:16,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 32it [00:16,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 32it [00:16,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 33it [00:17,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 33it [00:17,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 34it [00:17,  1.98it/s]\u001B[A\n",
      "Downloading page 1/1: : 34it [00:17,  1.98it/s]\u001B[A\n",
      "Downloading page 1/1: : 35it [00:18,  1.98it/s]\u001B[A\n",
      "Downloading page 1/1: : 35it [00:18,  1.98it/s]\u001B[A\n",
      "Downloading page 1/1: : 36it [00:18,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 36it [00:18,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 37it [00:19,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 37it [00:19,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 38it [00:19,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 38it [00:19,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 39it [00:20,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 39it [00:20,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 40it [00:20,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 40it [00:20,  1.96it/s]\u001B[A\n",
      "Downloading page 1/1: : 41it [00:21,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 41it [00:21,  1.97it/s]\u001B[A\n",
      "Downloading page 1/1: : 42it [00:22,  1.52it/s]\u001B[A\n",
      "Downloading page 1/1: : 42it [00:22,  1.52it/s]\u001B[A\n",
      "Downloading page 1/1: : 43it [00:22,  1.63it/s]\u001B[A\n",
      "Downloading page 1/1: : 43it [00:22,  1.63it/s]\u001B[A\n",
      "Downloading page 1/1: : 44it [00:23,  1.36it/s]\u001B[A\n",
      "Downloading page 1/1: : 44it [00:23,  1.36it/s]\u001B[A\n",
      "Downloading page 1/1: : 45it [00:24,  1.50it/s]\u001B[A\n",
      "Downloading page 1/1: : 45it [00:24,  1.50it/s]\u001B[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     19\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;66;03m# if link already exists in talks list, skip it\u001B[39;00m\n\u001B[1;32m     21\u001B[0m                 \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m             \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m pbar\u001B[38;5;241m.\u001B[39mclose()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "talks = [] # also called add_links\n",
    "\n",
    "# find href links to talks in page content under <a> tags\n",
    "pbar = tqdm(total=len(page_urls), dynamic_ncols=True, colour= 'green')\n",
    "for i, page in enumerate(page_urls):\n",
    "    page = requests.get(page_urls[i])\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    for link in soup.find_all('a'):\n",
    "        time.sleep(0.5)\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f'Downloading page {i+1}/{len(page_urls)}', refresh=True)\n",
    "\n",
    "        if link.has_attr('href') and link['href'].startswith('/talks/'):\n",
    "            ted_url = 'https://www.ted.com'\n",
    "            #check if link already exists in talks list\n",
    "            if ted_url + link['href'] not in talks:\n",
    "                #concat 'https://www.ted.com' + link['href'] to talks list\n",
    "                talks.append(ted_url + link['href'])\n",
    "            else:\n",
    "        # if link already exists in talks list, skip it\n",
    "                continue\n",
    "            time.sleep(0.5)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-28T19:09:30.419344Z"
    }
   },
   "outputs": [],
   "source": [
    "talks[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:10:16.275668Z",
     "start_time": "2023-06-28T19:10:16.268424Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# find views of talk under <div class=\"text-sm w-full truncate text-gray-900\" data-testid=\"talk-meta\">\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(\u001B[43mtalks\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m      3\u001B[0m soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(response\u001B[38;5;241m.\u001B[39mtext, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlxml\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m description_schema \u001B[38;5;241m=\u001B[39m soup\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhead\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmeta\u001B[39m\u001B[38;5;124m'\u001B[39m, attrs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdescription\u001B[39m\u001B[38;5;124m'\u001B[39m})[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# find views of talk under <div class=\"text-sm w-full truncate text-gray-900\" data-testid=\"talk-meta\">\n",
    "response = requests.get(talks[5])\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "description_schema = soup.find('head').find('meta', attrs={'name':'description'})['content']\n",
    "print (description_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading What caused the Rwandan Genocide? :   1%|\u001B[32m          \u001B[0m| 1/144 [00:30<1:12:10, 30.28s/it]\n",
      "/var/folders/7f/4z7lvktj44g121hm_1s6v18h0000gn/T/ipykernel_26163/4013203375.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'author': author, 'talk': talk, 'description': description, 'likes': likes, 'views': views}, ignore_index=True)\n",
      "/var/folders/7f/4z7lvktj44g121hm_1s6v18h0000gn/T/ipykernel_26163/4013203375.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'author': author, 'talk': talk, 'description': description, 'likes': likes, 'views': views}, ignore_index=True)\n",
      "/var/folders/7f/4z7lvktj44g121hm_1s6v18h0000gn/T/ipykernel_26163/4013203375.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'author': author, 'talk': talk, 'description': description, 'likes': likes, 'views': views}, ignore_index=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/SRC/scrapper.ipynb Cell 9\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/SRC/scrapper.ipynb#X10sZmlsZQ%3D%3D?line=50'>51</a>\u001B[0m pbar\u001B[39m.\u001B[39mset_description(\u001B[39mf\u001B[39m\u001B[39m'\u001B[39m\u001B[39mAdding \u001B[39m\u001B[39m{\u001B[39;00mtalk\u001B[39m}\u001B[39;00m\u001B[39m to database\u001B[39m\u001B[39m'\u001B[39m, refresh\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/SRC/scrapper.ipynb#X10sZmlsZQ%3D%3D?line=51'>52</a>\u001B[0m pbar\u001B[39m.\u001B[39mupdate(\u001B[39m1\u001B[39m)\n\u001B[0;32m---> <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/SRC/scrapper.ipynb#X10sZmlsZQ%3D%3D?line=53'>54</a>\u001B[0m time\u001B[39m.\u001B[39;49msleep(\u001B[39m1\u001B[39;49m)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/SRC/scrapper.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001B[0m cur\u001B[39m.\u001B[39mexecute(\u001B[39m\"\u001B[39m\u001B[39mSELECT * FROM talks\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/SRC/scrapper.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001B[0m rows \u001B[39m=\u001B[39m cur\u001B[39m.\u001B[39mfetchall()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=len(talks), dynamic_ncols=True, colour= 'green')\n",
    "\n",
    "for i, ad in enumerate(talks):\n",
    "    #-------create dataframe--------#\n",
    "    df = pd.DataFrame(columns=[\"author\", \"talk\", \"description\", \"likes\", \"views\"])\n",
    "\n",
    "    time.sleep(2)\n",
    "    pbar.update(1)\n",
    "    response = requests.get(talks[i])\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "\n",
    "    #--------Title Schema------------#\n",
    "    title_schema = soup.find('head').find('title').text.strip()\n",
    "\n",
    "    #--------Description Schema------------#\n",
    "    try:\n",
    "        description_schema = soup.find('head').find('meta', attrs={'name':'description'})['content'].strip()\n",
    "\n",
    "    except:\n",
    "        description_schema = ''\n",
    "\n",
    "    #--------Likes Schema------------#\n",
    "    likes_schema = soup.find_all('span')[0].get_text().strip()\n",
    "\n",
    "    #--------Views Schema------------#\n",
    "    views_schema = soup.find_all('div', class_='text-sm w-full truncate text-gray-900')\n",
    "\n",
    "\n",
    "    # get author name from title \n",
    "    author = title_schema.split(':')[0]\n",
    "    talk = title_schema.split(':')[1].strip().replace('| TED Talk', '')\n",
    "    description = description_schema\n",
    "    likes = likes_schema.replace('(', '').replace(')', '')\n",
    "    try: \n",
    "        views = views_schema[0].get_text().strip().split()[0]\n",
    "    except:\n",
    "        views = 0\n",
    "\n",
    "    pbar.set_description(f'Downloading {talk}', refresh=True)\n",
    "\n",
    "    # add to dataframe\n",
    "    df = df.append({'author': author, 'talk': talk, 'description': description, 'likes': likes, 'views': views}, ignore_index=True)\n",
    "\n",
    " # ----------------------------------------Saving to Database--------------------------------------------#\n",
    "    import sqlite3\n",
    "    conn = sqlite3.connect('talks.db')\n",
    "    cur = conn.cursor()\n",
    "                \n",
    "    for i in range(len(df)):\n",
    "        cur.execute(\"INSERT INTO talks (author, talk, description, likes, views) VALUES (?, ?, ?, ?, ?)\", (df.iloc[i]['author'], df.iloc[i]['talk'], df.iloc[i]['description'], df.iloc[i]['likes'], df.iloc[i]['views']))\n",
    "    conn.commit()\n",
    "\n",
    "    time.sleep(1)\n",
    "    pbar.set_description(f'Adding {talk} to database', refresh=True)\n",
    "    pbar.update(1)\n",
    "\n",
    "    time.sleep(1)\n",
    "    cur.execute(\"SELECT * FROM talks\")\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    pbar.set_description(f\"There are {len(rows)} records in the database\", refresh=True)\n",
    "    conn.close()\n",
    "    time.sleep(1)\n",
    "pbar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "deeplearning",
   "language": "python",
   "display_name": "DeepLearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
