{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:08:27.498648Z",
     "start_time": "2023-06-28T19:08:26.775363Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:09:32.869680Z",
     "start_time": "2023-06-28T19:09:32.862375Z"
    }
   },
   "outputs": [],
   "source": [
    "pages = [str(i) for i in range(0, 1, 1)] # page iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:09:36.176087Z",
     "start_time": "2023-06-28T19:09:36.165151Z"
    }
   },
   "outputs": [],
   "source": [
    "page_urls = []\n",
    "for page in pages:\n",
    "    base_url = 'https://www.ted.com/talks?page='+page\n",
    "    page_urls.append(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:09:37.916956Z",
     "start_time": "2023-06-28T19:09:37.908048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.ted.com/talks?page=0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:10:08.652021Z",
     "start_time": "2023-06-28T19:09:43.984651Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[32m          \u001b[0m| 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading page 1/1: : 151it [01:35,  1.58it/s]                   1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "talks = [] # also called add_links\n",
    "\n",
    "# find href links to talks in page content under <a> tags\n",
    "pbar = tqdm(total=len(page_urls), dynamic_ncols=True, colour= 'green')\n",
    "for i, page in enumerate(page_urls):\n",
    "    page = requests.get(page_urls[i])\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    for link in soup.find_all('a'):\n",
    "        time.sleep(0.5)\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f'Downloading page {i+1}/{len(page_urls)}', refresh=True)\n",
    "\n",
    "        if link.has_attr('href') and link['href'].startswith('/talks/'):\n",
    "            ted_url = 'https://www.ted.com'\n",
    "            #check if link already exists in talks list\n",
    "            if ted_url + link['href'] not in talks:\n",
    "                #concat 'https://www.ted.com' + link['href'] to talks list\n",
    "                talks.append(ted_url + link['href'])\n",
    "            else:\n",
    "        # if link already exists in talks list, skip it\n",
    "                continue\n",
    "            time.sleep(0.5)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-28T19:09:30.419344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.ted.com/talks/conor_russomanno_a_powerful_new_neurotech_tool_for_augmenting_your_mind'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talks[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:10:16.275668Z",
     "start_time": "2023-06-28T19:10:16.268424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I became obsessed with the relationship between the brain and the mind after suffering a series of concussions playing football and rugby in college. I felt my mind change for years after. I was studying computers at the time, and it felt as though I had damaged my hardware and that my software was running differently. Over the following years, a close friend suffered a serious neck injury and multiple friends and family members were struggling with crippling mental health issues. All around me, people that I loved dearly were being afflicted by ailments of the nervous system or the mind. I was grappling with all of this while pursuing an MFA in Design and Technology at Parsons when a friend and fellow student showed me an open-source tutorial on how to build a low-cost single-channel EEG system to detect brain activity. After a couple long nights of hacking and tinkering, I saw my brainwaves dancing across the screen for the very first time. And that moment changed my life. In that moment, I felt as though I had the possibility to help myself and the people I loved. And I also realized that I couldn&apos;t do it alone. I needed help. So in 2013, in Brooklyn, with some like-minded friends, I started OpenBCI, an open-source neurotechnology company. In the beginning, our goal was to build an inward-pointing telescope and to share the blueprints with the world so that anybody with a computer could begin peering into their own brain. At first, we were an EEG-only company. We sold brain sensors to measure brain activity. I thought that&apos;s what people wanted. But over time, we discovered people doing very strange things with our technology. Some people were connecting the equipment to the stomach to measure the neurons in the gut and study gut-brain connection and the microbiome. Others were using the tools to build new muscle sensors and controllers for prosthetics and robotics. And some were designing new devices and peripheral add-ons that could be connected to the platform to measure new types of data that I had never heard of before. What we learned from all of this is that the brain by itself is actually quite boring. Turns out brain data alone lacks context. And what we ultimately care about is not the brain, but the mind, consciousness, human cognition. When we have things like EMG sensors to measure muscle activity or ECG sensors to measure heart activity, eye trackers and even environmental sensors to measure the world around us, all of this makes the brain data much more useful. But the organs around our body, our sensory receptors, are actually much easier to collect data from than the brain, and also arguably much more important for determining the things that we actually care about: emotions, intentions and the mind overall. Additionally, we realized that people weren&apos;t just interested in reading from the brain and the body. They were also interested in modulating the mind through various types of sensory stimulation. Things like light, sound, haptics and electricity. It&apos;s one thing to record the mind, it&apos;s another to modulate it. The idea of a combined system that can both read from and write to the brain or body is referred to as a closed-loop system or bidirectional human interface. This concept is truly profound, and it will define the next major revolution in computing technology. When you have products that not just are designed for the average user but are designed to actually adapt to their user, that&apos;s something truly special. When we know what the data of an emotion or a feeling looks like and we know how to make that data go up or down, then using AI, we can build constructive or destructive interference patterns to either amplify or suppress those emotions or feelings. In the very near future, we will have computers that we are resonantly and subconsciously connected to, enabling empathetic computing for the very first time. In 2018, we put these learnings to work and began development of a new tool for cognitive exploration. Named after my friend Gael, who passed from ALS in 2016, we call it Galea. It’s a multimodal bio-sensing headset, and it is absolutely packed with sensors. It can measure the user’s heart, skin, muscles, eyes and brain, and it combines that capability with head-mounted displays or augmented and virtual reality headsets. Additionally, we&apos;re exploring the integration of non-invasive electrical neural stimulation as a feature. The Galea software suite can turn the raw sensor data into meaningful metrics. With some of the sensors, we&apos;re able to provide new forms of real-time interactivity and control. And with all of the sensors, we&apos;re able to make quantifiable inferences about high-level states of mind, things like stress, fatigue, cognitive workload and focus. In 2019, a legendary neurohacker by the name of Christian Bayerlein reached out to me. He was actually one of our very first Kickstarter backers when we got started, early on. Christian was a very smart, intelligent, happy-go-lucky and easygoing guy. And so I worked up the courage to ask him, &quot;Hey, Christian, can we connect you to our sensors?&quot; At which point he said, &quot;I thought you would never ask.&quot; (Laughter) So after 20 minutes, we had him rigged up to a bunch of electrodes, and we provided him with four new inputs to a computer. Little digital buttons, that he could control voluntarily. This essentially doubled his number of inputs to a computer. Years later, after many setbacks due to COVID, we flew to Germany to work with Christian in person to implement the first prototype of what we&apos;re going to be demoing here today. Christian then spent months training with that prototype and sending his data across the Atlantic to us in Brooklyn from Germany and flying a virtual drone in our offices. The first thing that we did was scour Christian&apos;s body for residual motor function. We then connected electrodes to the four muscles that he had the most voluntary control over, and then we turned those muscles into digital buttons. We then applied some smart filtering and signal processing to adapt those buttons into something more like a slider or a digital potentiometer. After that, we turned those four sliders and mapped them to a new virtual joystick. Christian then combined that new joystick with the joystick that he uses with his lip to control his wheelchair, and with the two joysticks combined, Christian finally had control over all the manual controls of a drone. I’m going to stop talking about it, and we’re going to show you. Christian, welcome. (Applause) At this point, I&apos;m going to ask everybody to turn off your Bluetooth and put your phones in airplane mode so that you don&apos;t get hit in the face with a drone. (Laughter) How are you feeling, Christian? Christian Bayerlein: Yeah, let&apos;s do it. Conor Russomanno: Awesome. This is a heads-up display that&apos;s showing all of Christian&apos;s biometric data, as well as some information about the drone. On the left here, we can see Christian&apos;s muscle data. Christian is now going to attempt to fly the drone. How are you feeling, Christian, feeling good? CB: Yes. CR: All right. Rock and roll. Let&apos;s take this up for a joyride. Whenever you&apos;re ready. CB: I&apos;m ready. (Applause and cheers) CR: All right, take her up. And now let&apos;s do something we probably shouldn&apos;t do and fly it over the audience. (Laughter) (Cheers and applause) Alright, actually, let’s do this. I&apos;m going to ask for people to call out some commands in the audience. So how about you? Straight forward. Straight forward. (Laughter) Alright. How about you? Man: Up! (Laughter) CR: Not down. Oh, he&apos;s doing what he wants right now. Amazing. (Cheers and applause) Alright, let’s bring it back. And what I&apos;m going to do right now is take control of the controller so that you guys know that there isn&apos;t someone backstage flying this drone. All right, Christian, you&apos;re alright with that? CB: Yeah. CR: Unplug. Forward. And we&apos;re going to land this guy now. CB: I think I was better than you. (Laughter) (Applause) CR: Amazing. (Applause) Now I&apos;m going to unplug it so it doesn&apos;t turn on on its own. Perfect. Christian has repurposed dormant muscles from around his body for extended and augmented interactivity. We have turned those muscles into a generic controller that in this case we&apos;ve mapped into a drone, but what&apos;s really cool is that joystick can be applied to anything. Another thing that&apos;s really cool is that even in individuals who are not living with motor disabilities, there exist dozens of dormant muscles around the body that we can tap into for augmented and expanded control interactivity. And lastly, all the code related to that virtual joystick, we&apos;re going to open source so that you can implement it and improve upon it. There&apos;s three things that have stood out to me from working on this project and many others over the years. One, we cannot conflate the brain with the mind. In order to understand emotions and tensions and the mind overall, we have to measure data from all over the body, not just the brain. Two, open-source technology access and literacy is one way that we can combat the potential ethical challenges we face in introducing neural technology to society. But that&apos;s not enough. We have to do much, much more than that. It&apos;s very important, imperative, that we set up guardrails and design the future that we want to live in. Three. It&apos;s the courage and resilience of trailblazers like Christian who don&apos;t get bogged down by what they can&apos;t do, but instead strive to prove that the impossible is in fact possible. (Applause) And since none of this would have been possible without you, Christian, the stage is yours. CB: Yeah, hi, everybody. Audience: Hi. CB: I&apos;m excited to be here today. I was born with a genetic condition that affects my mobility and requires me to have assistance. Despite my disability, I&apos;m a very happy and fulfilled person. What truly holds me back are not my physical limitations. It&apos;s rather the barriers in the environment. I&apos;m a tech nerd and political activist. I believe that technology can empower disabled people. It can help create a better, more inclusive and accessible world for everyone. This demonstration is a perfect example. We saw what&apos;s possible when cutting edge technology is combined with human curiosity and creativity. So let&apos;s build tools that empower people, applications that break down barriers and systems that unlock a world of possibilities. I think that&apos;s an idea worth spreading. Thank you. (Cheers and applause)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# find views of talk under <div class=\"text-sm w-full truncate text-gray-900\" data-testid=\"talk-meta\">\n",
    "response = requests.get(talks[5])\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "schema = soup.find('head').find('script', type='application/ld+json').text\n",
    "schema = json.loads(schema)\n",
    "schema.get('transcript')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading What caused the Rwandan Genocide? :   1%|\u001b[32m          \u001b[0m| 1/144 [00:30<1:12:10, 30.28s/it]\n",
      "/var/folders/7f/4z7lvktj44g121hm_1s6v18h0000gn/T/ipykernel_26163/4013203375.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'author': author, 'talk': talk, 'description': description, 'likes': likes, 'views': views}, ignore_index=True)\n",
      "/var/folders/7f/4z7lvktj44g121hm_1s6v18h0000gn/T/ipykernel_26163/4013203375.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'author': author, 'talk': talk, 'description': description, 'likes': likes, 'views': views}, ignore_index=True)\n",
      "/var/folders/7f/4z7lvktj44g121hm_1s6v18h0000gn/T/ipykernel_26163/4013203375.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'author': author, 'talk': talk, 'description': description, 'likes': likes, 'views': views}, ignore_index=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/SRC/scrapper.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/SRC/scrapper.ipynb#X10sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m pbar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAdding \u001b[39m\u001b[39m{\u001b[39;00mtalk\u001b[39m}\u001b[39;00m\u001b[39m to database\u001b[39m\u001b[39m'\u001b[39m, refresh\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/SRC/scrapper.ipynb#X10sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/SRC/scrapper.ipynb#X10sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/SRC/scrapper.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m cur\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mSELECT * FROM talks\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/SRC/scrapper.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m rows \u001b[39m=\u001b[39m cur\u001b[39m.\u001b[39mfetchall()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=len(talks), dynamic_ncols=True, colour= 'green')\n",
    "\n",
    "for i, ad in enumerate(talks):\n",
    "    #-------create dataframe--------#\n",
    "    df = pd.DataFrame(columns=[\"author\", \"talk\", \"description\", \"likes\", \"views\"])\n",
    "\n",
    "    time.sleep(2)\n",
    "    pbar.update(1)\n",
    "    response = requests.get(talks[i])\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "\n",
    "    #--------Title Schema------------#\n",
    "    title_schema = soup.find('head').find('title').text.strip()\n",
    "\n",
    "    #--------Description Schema------------#\n",
    "    try:\n",
    "        description_schema = soup.find('head').find('meta', attrs={'name':'description'})['content'].strip()\n",
    "\n",
    "    except:\n",
    "        description_schema = ''\n",
    "\n",
    "    #--------Likes Schema------------#\n",
    "    likes_schema = soup.find_all('span')[0].get_text().strip()\n",
    "\n",
    "    #--------Views Schema------------#\n",
    "    views_schema = soup.find_all('div', class_='text-sm w-full truncate text-gray-900')\n",
    "\n",
    "\n",
    "    # get author name from title \n",
    "    author = title_schema.split(':')[0]\n",
    "    talk = title_schema.split(':')[1].strip().replace('| TED Talk', '')\n",
    "    description = description_schema\n",
    "    likes = likes_schema.replace('(', '').replace(')', '')\n",
    "    try: \n",
    "        views = views_schema[0].get_text().strip().split()[0]\n",
    "    except:\n",
    "        views = 0\n",
    "\n",
    "    pbar.set_description(f'Downloading {talk}', refresh=True)\n",
    "\n",
    "    # add to dataframe\n",
    "    df = df.append({'author': author, 'talk': talk, 'description': description, 'likes': likes, 'views': views}, ignore_index=True)\n",
    "\n",
    " # ----------------------------------------Saving to Database--------------------------------------------#\n",
    "    import sqlite3\n",
    "    conn = sqlite3.connect('talks.db')\n",
    "    cur = conn.cursor()\n",
    "                \n",
    "    for i in range(len(df)):\n",
    "        cur.execute(\"INSERT INTO talks (author, talk, description, likes, views) VALUES (?, ?, ?, ?, ?)\", (df.iloc[i]['author'], df.iloc[i]['talk'], df.iloc[i]['description'], df.iloc[i]['likes'], df.iloc[i]['views']))\n",
    "    conn.commit()\n",
    "\n",
    "    time.sleep(1)\n",
    "    pbar.set_description(f'Adding {talk} to database', refresh=True)\n",
    "    pbar.update(1)\n",
    "\n",
    "    time.sleep(1)\n",
    "    cur.execute(\"SELECT * FROM talks\")\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    pbar.set_description(f\"There are {len(rows)} records in the database\", refresh=True)\n",
    "    conn.close()\n",
    "    time.sleep(1)\n",
    "pbar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
