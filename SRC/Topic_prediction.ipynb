{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>talk</th>\n",
       "      <th>description</th>\n",
       "      <th>likes</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conor Russomanno</td>\n",
       "      <td>a powerful new neurotech tool for augmenting y...</td>\n",
       "      <td>in an astonishing talk and tech demo neurotech...</td>\n",
       "      <td>4700</td>\n",
       "      <td>157930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peter Singer</td>\n",
       "      <td>a modern argument for the rights of animals</td>\n",
       "      <td>why do we prioritize human rights over those o...</td>\n",
       "      <td>7600</td>\n",
       "      <td>254482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sahar Zand</td>\n",
       "      <td>why iranians are cutting their hair for woman ...</td>\n",
       "      <td>filmmaker sahar zand vividly explores the ongo...</td>\n",
       "      <td>1100</td>\n",
       "      <td>393882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shannon Odell</td>\n",
       "      <td>are solar panels worth it</td>\n",
       "      <td>today in many countries solar is the cheapest ...</td>\n",
       "      <td>3700</td>\n",
       "      <td>126251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angus Hervey</td>\n",
       "      <td>why are we so bad at reporting good news</td>\n",
       "      <td>why is good news so rare in a special broadcas...</td>\n",
       "      <td>1200</td>\n",
       "      <td>415329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                               talk  \\\n",
       "0  Conor Russomanno  a powerful new neurotech tool for augmenting y...   \n",
       "1      Peter Singer        a modern argument for the rights of animals   \n",
       "2        Sahar Zand  why iranians are cutting their hair for woman ...   \n",
       "3     Shannon Odell                          are solar panels worth it   \n",
       "4      Angus Hervey           why are we so bad at reporting good news   \n",
       "\n",
       "                                         description  likes   views  \n",
       "0  in an astonishing talk and tech demo neurotech...   4700  157930  \n",
       "1  why do we prioritize human rights over those o...   7600  254482  \n",
       "2  filmmaker sahar zand vividly explores the ongo...   1100  393882  \n",
       "3  today in many countries solar is the cheapest ...   3700  126251  \n",
       "4  why is good news so rare in a special broadcas...   1200  415329  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = '/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/Data_output/talks.csv'\n",
    "df = pd.read_csv(data, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/patrickokwir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------Imports--------------------------------------------------#\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import nlp\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#--------------------------------------------------Things to Note--------------------------------------------------#\n",
    "#The following are key factors to obtaining good segregation topics:**\n",
    "\n",
    "#The quality of text processing.\n",
    "#The variety of topics the text talks about.\n",
    "#The choice of topic modeling algorithm.\n",
    "#The number of topics fed to the algorithm.\n",
    "#The algorithms tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:10: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:7: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:10: DeprecationWarning: invalid escape sequence \\s\n",
      "/var/folders/7f/4z7lvktj44g121hm_1s6v18h0000gn/T/ipykernel_42214/4165407693.py:7: DeprecationWarning: invalid escape sequence \\S\n",
      "  data = [re.sub('\\S*@\\S*\\s?', '', i) for i in data]\n",
      "/var/folders/7f/4z7lvktj44g121hm_1s6v18h0000gn/T/ipykernel_42214/4165407693.py:10: DeprecationWarning: invalid escape sequence \\s\n",
      "  data = [re.sub('\\s+', ' ', i) for i in data]\n"
     ]
    }
   ],
   "source": [
    "#___________________________________________________Data Processing__________________________________________________#\n",
    "\n",
    "# Convert to list\n",
    "data = df['description'].values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', i) for i in data]\n",
    "\n",
    "# Remove new line characters in data\n",
    "data = [re.sub('\\s+', ' ', i) for i in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", i) for i in data]\n",
    "\n",
    "# import pprint\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['in', 'an', 'astonishing', 'talk', 'and', 'tech', 'demo', 'conor', 'russomanno', 'shares', 'his', 'work', 'building', 'braincomputer', 'interfaces', 'that', 'could', 'enable', 'us', 'to', 'control', 'the', 'external', 'world', 'with', 'our', 'minds', 'he', 'discusses', 'the', 'quickly', 'advancing', 'possibilities', 'of', 'this', 'field', 'including', 'the', 'promise', 'of', 'closedloop', 'system', 'that', 'could', 'both', 'record', 'and', 'stimulate', 'brain', 'activity', 'and', 'invites', 'neurohacker', 'christian', 'bayerlein', 'onto', 'the', 'ted', 'stage', 'to', 'fly', 'mindcontrolled', 'drone', 'by', 'using', 'biosensing', 'headset']]\n"
     ]
    }
   ],
   "source": [
    "#break down each sentence into a list of words through tokenization\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_newline(series):\n",
    "    return [review.replace('\\n','') for review in series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['music', 'stephen', 'detail', 'way', 'shares', 'need', 'us', 'come','order','try','go','get','make','drink','plate','dish','restaurant','place', 'many', 'day', 'explains', 'even', 'part',\n",
    "                  'would','really','like','great','service','came','got', 'talk', 'directed', 'ted', 'narrated', 'new', 'one', 'using', 'addison', 'anderson', 'says', \"addison_anderson\", \"years\", \"first\", \n",
    "                  'know', 'actually', 'worlds', 'could', 'details', 'studio', 'help', 'music', 'life', 'shows', 'world', 'good', 'think'])\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def bigrams(words, bi_min=10, tri_min=7):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "\n",
    "# def get_corpus(df):\n",
    "#     df['text'] = df['review']\n",
    "#     df['text'] = strip_newline(df.text)\n",
    "#     words = list(sent_to_words(df.text))\n",
    "#     words = remove_stopwords(words)\n",
    "#     bigram_mod = bigrams(words)\n",
    "#     bigram = [bigram_mod[review] for review in words]\n",
    "#     id2word = gensim.corpora.Dictionary(bigram)\n",
    "#     id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "#     id2word.compactify()\n",
    "#     corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "#     return corpus, id2word, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = remove_stopwords(data_words)\n",
    "bigram = bigrams(words)\n",
    "bigram = [bigram[review] for review in words]\n",
    "id2word = gensim.corpora.Dictionary(bigram)\n",
    "id2word.filter_extremes(no_below=5, no_above=0.1)\n",
    "id2word.compactify()\n",
    "corpus = [id2word.doc2bow(text) for text in bigram]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus4 = corpus\n",
    "train_id2word4 = id2word\n",
    "bigram_train4 = bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    lda_train4 = gensim.models.ldamulticore.LdaMulticore(\n",
    "                           corpus=train_corpus4,\n",
    "                           num_topics=8,\n",
    "                           id2word=train_id2word4,\n",
    "                           chunksize=100,\n",
    "                           workers=7, # Num. Processing Cores - 1\n",
    "                           passes=50,\n",
    "                           eval_every = 1,\n",
    "                           per_word_topics=True)\n",
    "    # lda_train4.save('lda_train4.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36194914273489537"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_train4, texts=bigram_train4, dictionary=train_id2word4, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "coherence_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"women\" + 0.008*\"stories\" + 0.008*\"love\" + 0.007*\"story\" + 0.006*\"powerful\"'),\n",
       " (1,\n",
       "  '0.007*\"art\" + 0.006*\"human\" + 0.005*\"see\" + 0.005*\"work\" + 0.005*\"design\"'),\n",
       " (2,\n",
       "  '0.012*\"research\" + 0.010*\"disease\" + 0.008*\"cancer\" + 0.007*\"brain\" + 0.007*\"health\"'),\n",
       " (3,\n",
       "  '0.006*\"planet\" + 0.005*\"water\" + 0.005*\"humanity\" + 0.005*\"future\" + 0.005*\"ocean\"'),\n",
       " (4,\n",
       "  '0.007*\"global\" + 0.006*\"work\" + 0.006*\"business\" + 0.005*\"create\" + 0.005*\"social\"'),\n",
       " (5,\n",
       "  '0.009*\"food\" + 0.004*\"water\" + 0.004*\"amazing\" + 0.004*\"theres\" + 0.004*\"system\"'),\n",
       " (6,\n",
       "  '0.008*\"city\" + 0.005*\"global\" + 0.005*\"history\" + 0.005*\"ancient\" + 0.005*\"earth\"'),\n",
       " (7,\n",
       "  '0.009*\"time\" + 0.007*\"work\" + 0.004*\"human\" + 0.004*\"future\" + 0.004*\"better\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train4.print_topics(10,num_words=5)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "Words: ['women', 'stories', 'love', 'story', 'powerful', 'personal', 'tells', 'children', 'two', 'family']\n",
      "Weights: [0.00786605, 0.0078099687, 0.00770874, 0.0071252896, 0.0058093467, 0.005576617, 0.0048611667, 0.0047532325, 0.0044897012, 0.004318806]\n",
      "\n",
      "\n",
      "Topic 1\n",
      "Words: ['art', 'human', 'see', 'work', 'design', 'science', 'space', 'technology', 'artist', 'language']\n",
      "Weights: [0.0065333364, 0.005576193, 0.0054030404, 0.005280138, 0.0052148825, 0.0051552043, 0.004904514, 0.0042758016, 0.0041206265, 0.0040687593]\n",
      "\n",
      "\n",
      "Topic 2\n",
      "Words: ['research', 'disease', 'cancer', 'brain', 'health', 'medical', 'de', 'better', 'science', 'diseases']\n",
      "Weights: [0.012373014, 0.009794811, 0.007653066, 0.006860262, 0.006642337, 0.0064943205, 0.0057874816, 0.005661902, 0.004217067, 0.0042109233]\n",
      "\n",
      "\n",
      "Topic 3\n",
      "Words: ['planet', 'water', 'humanity', 'future', 'ocean', 'global', 'ways', 'protect', 'plastic', 'may']\n",
      "Weights: [0.006154424, 0.005258608, 0.004967137, 0.00472105, 0.0047130347, 0.004705911, 0.0043611135, 0.004358355, 0.004328784, 0.004270753]\n",
      "\n",
      "\n",
      "Topic 4\n",
      "Words: ['global', 'work', 'business', 'create', 'social', 'build', 'power', 'future', 'change', 'education']\n",
      "Weights: [0.006796007, 0.006404736, 0.0061430614, 0.0049510766, 0.004864119, 0.0048378357, 0.0046165045, 0.0046117054, 0.0043814993, 0.004267759]\n",
      "\n",
      "\n",
      "Topic 5\n",
      "Words: ['food', 'water', 'amazing', 'theres', 'system', 'doctors', 'every', 'lives', 'dance', 'brain']\n",
      "Weights: [0.009466935, 0.0041672713, 0.0041500484, 0.0040862933, 0.004035367, 0.0039420626, 0.0038594669, 0.0036810418, 0.0035192918, 0.003505351]\n",
      "\n",
      "\n",
      "Topic 6\n",
      "Words: ['city', 'global', 'history', 'ancient', 'earth', 'today', 'two', 'climate', 'year', 'set']\n",
      "Weights: [0.0077408706, 0.0054916986, 0.00532232, 0.004854576, 0.004826328, 0.0045315605, 0.004426535, 0.004375653, 0.004182949, 0.0038796458]\n",
      "\n",
      "\n",
      "Topic 7\n",
      "Words: ['time', 'work', 'human', 'future', 'better', 'less', 'job', 'ideas', 'online', 'change']\n",
      "Weights: [0.008732399, 0.006969581, 0.0037836512, 0.0035938984, 0.0035227702, 0.0034894717, 0.003473346, 0.0033823913, 0.0033587697, 0.0032042866]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exract topics from the model\n",
    "topics = lda_train4.show_topics(formatted=False)\n",
    "for topic in topics:\n",
    "    print(\"Topic\", topic[0])\n",
    "    print(\"Words:\", [word[0] for word in topic[1]])\n",
    "    print(\"Weights:\", [word[1] for word in topic[1]])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/patrickokwir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary resources for tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Assuming you have already trained your LDA model and have it stored in the 'lda_model' variable\n",
    "\n",
    "# Create a list to store the extracted topics\n",
    "extracted_topics = []\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    # Extract the text from the appropriate column in your dataframe\n",
    "    text = row['description']\n",
    "    \n",
    "    # Tokenize the text into individual tokens\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Preprocess the tokens if necessary (e.g., removing stopwords, stemming, etc.)\n",
    "    \n",
    "    # Convert the tokens to a bag-of-words representation\n",
    "    bow_vector = lda_train4.id2word.doc2bow(tokens)\n",
    "    \n",
    "    # Get the topic distribution for the document\n",
    "    topic_distribution = lda_train4.get_document_topics(bow_vector)\n",
    "    \n",
    "    # Sort the topics by their probability in descending order\n",
    "    sorted_topics = sorted(topic_distribution, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Extract the most probable topic (assuming the topics are represented as tuples (topic_id, probability))\n",
    "    most_probable_topic = sorted_topics[0][0]\n",
    "    if most_probable_topic == 0:\n",
    "        most_probable_topic = \"Women's Rights\"\n",
    "    elif most_probable_topic == 1:\n",
    "        most_probable_topic = \"Art, Science, and Technology\"\n",
    "    elif most_probable_topic == 2:\n",
    "        most_probable_topic = \"Medicine and Health\"\n",
    "    elif most_probable_topic == 3:\n",
    "        most_probable_topic = \"Oceans and Clean Water\"\n",
    "    elif most_probable_topic == 4:\n",
    "        most_probable_topic = \"Bussiness and Economics\"\n",
    "    elif most_probable_topic == 5:\n",
    "        most_probable_topic = \"Food, Agriculture, and Water\"\n",
    "    elif most_probable_topic == 6:\n",
    "        most_probable_topic = \"History and Culture\"\n",
    "    elif most_probable_topic == 7:\n",
    "        most_probable_topic = \"Social Life\"\n",
    "    # Append the most probable topic to the extracted_topics list\n",
    "    extracted_topics.append(most_probable_topic)\n",
    "df['Topic'] = extracted_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>talk</th>\n",
       "      <th>description</th>\n",
       "      <th>likes</th>\n",
       "      <th>views</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Conor Russomanno</td>\n",
       "      <td>a powerful new neurotech tool for augmenting y...</td>\n",
       "      <td>in an astonishing talk and tech demo neurotech...</td>\n",
       "      <td>4700</td>\n",
       "      <td>157930</td>\n",
       "      <td>Social Life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peter Singer</td>\n",
       "      <td>a modern argument for the rights of animals</td>\n",
       "      <td>why do we prioritize human rights over those o...</td>\n",
       "      <td>7600</td>\n",
       "      <td>254482</td>\n",
       "      <td>Medicine and Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sahar Zand</td>\n",
       "      <td>why iranians are cutting their hair for woman ...</td>\n",
       "      <td>filmmaker sahar zand vividly explores the ongo...</td>\n",
       "      <td>1100</td>\n",
       "      <td>393882</td>\n",
       "      <td>Women's Rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shannon Odell</td>\n",
       "      <td>are solar panels worth it</td>\n",
       "      <td>today in many countries solar is the cheapest ...</td>\n",
       "      <td>3700</td>\n",
       "      <td>126251</td>\n",
       "      <td>Oceans and Clean Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angus Hervey</td>\n",
       "      <td>why are we so bad at reporting good news</td>\n",
       "      <td>why is good news so rare in a special broadcas...</td>\n",
       "      <td>1200</td>\n",
       "      <td>415329</td>\n",
       "      <td>Women's Rights</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                               talk  \\\n",
       "0  Conor Russomanno  a powerful new neurotech tool for augmenting y...   \n",
       "1      Peter Singer        a modern argument for the rights of animals   \n",
       "2        Sahar Zand  why iranians are cutting their hair for woman ...   \n",
       "3     Shannon Odell                          are solar panels worth it   \n",
       "4      Angus Hervey           why are we so bad at reporting good news   \n",
       "\n",
       "                                         description  likes   views  \\\n",
       "0  in an astonishing talk and tech demo neurotech...   4700  157930   \n",
       "1  why do we prioritize human rights over those o...   7600  254482   \n",
       "2  filmmaker sahar zand vividly explores the ongo...   1100  393882   \n",
       "3  today in many countries solar is the cheapest ...   3700  126251   \n",
       "4  why is good news so rare in a special broadcas...   1200  415329   \n",
       "\n",
       "                    Topic  \n",
       "0             Social Life  \n",
       "1     Medicine and Health  \n",
       "2          Women's Rights  \n",
       "3  Oceans and Clean Water  \n",
       "4          Women's Rights  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
