{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>likes</th>\n",
       "      <th>views</th>\n",
       "      <th>transcript</th>\n",
       "      <th>date</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Dazzle</td>\n",
       "      <td>how to unleash your inner maximalist through c...</td>\n",
       "      <td>tapping into the transformational power of cos...</td>\n",
       "      <td>8100</td>\n",
       "      <td>270192</td>\n",
       "      <td>Hello, I am Machine Dazzle, and I am an emotio...</td>\n",
       "      <td>Jun 2023</td>\n",
       "      <td>art, creativity, design, fashion, performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jioji Ravulo</td>\n",
       "      <td>a liberating vision of identity that transcend...</td>\n",
       "      <td>how can we move past societys inclination to b...</td>\n",
       "      <td>9200</td>\n",
       "      <td>309952</td>\n",
       "      <td>Can you paint with all the colors of the wind?...</td>\n",
       "      <td>Jun 2023</td>\n",
       "      <td>diversity, identity, inclusion, indigenous_peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rebecca Darwent</td>\n",
       "      <td>how to fund real change in your community</td>\n",
       "      <td>is there a way to give back that benefits ever...</td>\n",
       "      <td>1000</td>\n",
       "      <td>341218</td>\n",
       "      <td>I spent my whole career in the nonprofit secto...</td>\n",
       "      <td>Jun 2023</td>\n",
       "      <td>business, community, equality, humanity, money...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Susanne Buckley-Zistel</td>\n",
       "      <td>what caused the rwandan genocide</td>\n",
       "      <td>for one hundred days in 1994 the african count...</td>\n",
       "      <td>3700</td>\n",
       "      <td>126376</td>\n",
       "      <td>For 100 days in 1994, the African country of R...</td>\n",
       "      <td>Jun 2023</td>\n",
       "      <td>africa, animation, education, history, identit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conor Russomanno</td>\n",
       "      <td>a powerful new neurotech tool for augmenting y...</td>\n",
       "      <td>in an astonishing talk and tech demo neurotech...</td>\n",
       "      <td>1100</td>\n",
       "      <td>374259</td>\n",
       "      <td>I became obsessed with the relationship betwee...</td>\n",
       "      <td>Jun 2023</td>\n",
       "      <td>biotech, brain, disability, health, invention,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author                                              title  \\\n",
       "0          Machine Dazzle  how to unleash your inner maximalist through c...   \n",
       "1            Jioji Ravulo  a liberating vision of identity that transcend...   \n",
       "2         Rebecca Darwent          how to fund real change in your community   \n",
       "3  Susanne Buckley-Zistel                   what caused the rwandan genocide   \n",
       "4        Conor Russomanno  a powerful new neurotech tool for augmenting y...   \n",
       "\n",
       "                                         description  likes   views  \\\n",
       "0  tapping into the transformational power of cos...   8100  270192   \n",
       "1  how can we move past societys inclination to b...   9200  309952   \n",
       "2  is there a way to give back that benefits ever...   1000  341218   \n",
       "3  for one hundred days in 1994 the african count...   3700  126376   \n",
       "4  in an astonishing talk and tech demo neurotech...   1100  374259   \n",
       "\n",
       "                                          transcript      date  \\\n",
       "0  Hello, I am Machine Dazzle, and I am an emotio...  Jun 2023   \n",
       "1  Can you paint with all the colors of the wind?...  Jun 2023   \n",
       "2  I spent my whole career in the nonprofit secto...  Jun 2023   \n",
       "3  For 100 days in 1994, the African country of R...  Jun 2023   \n",
       "4  I became obsessed with the relationship betwee...  Jun 2023   \n",
       "\n",
       "                                                tags  \n",
       "0      art, creativity, design, fashion, performance  \n",
       "1  diversity, identity, inclusion, indigenous_peo...  \n",
       "2  business, community, equality, humanity, money...  \n",
       "3  africa, animation, education, history, identit...  \n",
       "4  biotech, brain, disability, health, invention,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = '/Users/patrickokwir/Desktop/Git_Projects/Ted-Talks-Recommender-System/Data_output/ted_talk_clean_merged_bert.csv'\n",
    "df = pd.read_csv(data, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = df.index.tolist()\n",
    "df_rows = df_rows[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate pipeline \n",
    "def process_data(df):\n",
    "    #instantiate a pipeline step\n",
    "    tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    #instantiate a pipeline step\n",
    "    model = AutoModel.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    #instantiate a pipeline step\n",
    "    tokens = {'input_ids': [], 'attention_mask': []}\n",
    "    for row in df_rows:\n",
    "        new_tokens = tokenizer.encode_plus(df['transcript'][row],\n",
    "                                    max_length=512,\n",
    "                                    truncation=True,\n",
    "                                    padding='max_length',\n",
    "                                    return_tensors='pt')\n",
    "        tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "        tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "    tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "    tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
    "\n",
    "    outputs = model(**tokens)\n",
    "    outputs.keys()\n",
    "\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    embeddings.shape\n",
    "\n",
    "    #mean pooling\n",
    "    mask = tokens['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float()\n",
    "\n",
    "    #mask embeddings\n",
    "    mask_embeddings = embeddings * mask\n",
    "\n",
    "    #summed embeddings\n",
    "    summed = torch.sum(mask_embeddings, dim=1)\n",
    "\n",
    "    #counts\n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "\n",
    "    #mean pooled \n",
    "    mean_pooled = summed/counts\n",
    "\n",
    "    mean_pooled = mean_pooled.detach().numpy()\n",
    "\n",
    "    # calculate cosine similarity for all rows using mean_pooled and cosine similarity\n",
    "    cosine_sim = cosine_similarity(mean_pooled, mean_pooled)\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "transformer = FunctionTransformer(process_data, validate=False)\n",
    "pipe = Pipeline([('transformer', transformer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cosine_sim = pipe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "pickle.dump(cosine_sim, open(\"test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open saved model\n",
    "cosine_sim = pickle.load(open(\"test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, create a reverse map of indices and descriptions\n",
    "indices = pd.Series(df.index, index=df['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now we create a function recomender that will recommend simmilar products, function must take item_id and count as parameters\n",
    "\n",
    "# # talk_to_search = 'are solar panels worth it'\n",
    "# # top_n_results = 3\n",
    "\n",
    "# def recomender(talk_to_search, top_n_results):\n",
    "   \n",
    "#     count = top_n_results\n",
    "\n",
    "#     id = df.loc[df['title'] == talk_to_search].index.values[0]\n",
    "#        # Get the index of the item that matches the title\n",
    "#     idx = indices[id]\n",
    "\n",
    "#     # Get the pairwsie similarity scores of all movies with that talk\n",
    "#     sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "#     # Sort the movies based on the similarity scores\n",
    "#     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=False)\n",
    "\n",
    "#     # Get the scores of the 10 most similar talks\n",
    "#     sim_scores = sim_scores[1:count+1]\n",
    "\n",
    "#     # Get the talk indices\n",
    "#     item_indicies = [i[0] for i in sim_scores]\n",
    "\n",
    "#     # Return the top 10 most similar talks\n",
    "#     top_talks_idx = df['transcript'].iloc[item_indicies].index[:count]\n",
    "#     # get author, talk using top_talks_idx\n",
    "#     top_talks_author = df['author'].iloc[item_indicies].values[:count]\n",
    "#     top_talks_talk = df['title'].iloc[item_indicies].values[:count]\n",
    "#     # get similarity scores using top_talks_idx\n",
    "#     top_n_results_sim_scores = [list(enumerate(cosine_sim[i]))[1][1] for i in top_talks_idx]\n",
    "\n",
    "   \n",
    "\n",
    "#     # create a result df \n",
    "#     result_df = pd.DataFrame({'author': top_talks_author, 'title': top_talks_talk, 'sim score': top_n_results_sim_scores})\n",
    "#     result_df = result_df.sort_values(by='sim score', ascending=False)\n",
    "\n",
    "#     # rename columns\n",
    "#     result_df.columns = ['author', 'title', 'sim score']\n",
    "\n",
    "#     return result_df\n",
    "\n",
    "\n",
    "\n",
    "#    # use indices to get 'author' and 'talk' columns from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender(talk_to_search, top_n_results):\n",
    "    count = top_n_results\n",
    "\n",
    "    talk_indices = df[df['title'] == talk_to_search].index.values\n",
    "    if len(talk_indices) == 0:\n",
    "        print(\"Talk not found in DataFrame\")\n",
    "        return None\n",
    "\n",
    "    id = talk_indices[0]\n",
    "    if id >= len(indices):\n",
    "        print(\"Invalid talk index\")\n",
    "        return None\n",
    "\n",
    "    idx = indices[id]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=False)\n",
    "    sim_scores = sim_scores[1:count+1]\n",
    "    item_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Filter out invalid indices\n",
    "    item_indices = [i for i in item_indices if i < len(df)]\n",
    "\n",
    "    top_talks_author = df['author'].iloc[item_indices].values[:count]\n",
    "    top_talks_talk = df['title'].iloc[item_indices].values[:count]\n",
    "    top_n_results_sim_scores = [list(enumerate(cosine_sim[i]))[1][1] for i in item_indices]\n",
    "\n",
    "    result_df = pd.DataFrame({'author': top_talks_author, 'title': top_talks_talk, 'sim score': top_n_results_sim_scores})\n",
    "    result_df = result_df.sort_values(by='sim score', ascending=False)\n",
    "    result_df.columns = ['author', 'title', 'sim score']\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get title of talk at index 25\n",
    "df[df['author'] == 'Conor Russomanno']['title'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender('a powerful new neurotech tool for augmenting your mind', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
